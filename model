import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import math
from torch.utils.data import Dataset, DataLoader
from Bio import SeqIO
import numpy as np
from tensorboardX import SummaryWriter


# ---------------------- 保留原有Encoder相关类（无需改） ----------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
       
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)
        attn_probs = torch.softmax(attn_scores, dim=-1)
        output = torch.matmul(attn_probs, V)
        return output
       
    def split_heads(self, x):
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
       
    def combine_heads(self, x):
        batch_size, _, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
       
    def forward(self, Q, K, V, mask=None):
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)
        output = self.W_o(self.combine_heads(attn_output))
        return output
        
class PositionWiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(PositionWiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
       
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
       
    def forward(self, x, mask):
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        return x


class TransformerClassifier(nn.Module):  
    def __init__(self, src_vocab_size=21, d_model=128, num_heads=4, num_layers=6, d_ff=512, max_seq_length=128, dropout=0.1, num_classes=2):
        super(TransformerClassifier, self).__init__()
        self.src_vocab_size = src_vocab_size
        self.d_model = d_model
        self.num_classes = num_classes 
       
        # 只保留Encoder相关组件
        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # 源序列嵌入
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
       
   
        self.dropout = nn.Dropout(dropout)
        self.vgg1 = nn.Sequential(
                                  nn.Conv2d(1,64,3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.MaxPool2d(2,stride=2))
        self.vgg2 = nn.Sequential(nn.Conv2d(64,128,3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.MaxPool2d(2,stride=2))
        self.vgg3 = nn.Sequential(nn.Conv2d(128,256,3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.MaxPool2d(2,stride=2))
        self.vgg4 = nn.Sequential(nn.Conv2d(256,512,3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),
                                  nn.ReLU(),
                                  nn.MaxPool2d(2,stride=2))
        self.vggfc1 = nn.Sequential(nn.Flatten(),
                                  nn.Linear(32768,4096),
                                  nn.ReLU(),
                                  nn.Dropout(0.5))
        self.vggfc2 = nn.Sequential(
                                  nn.Linear(4096,4096),
                                  nn.ReLU(),
                                  nn.Dropout(0.5))
        self.vggfc3 = nn.Sequential(nn.Linear(4096,2048),nn.ReLU(),
                                    nn.Linear(2048,512),nn.ReLU(),nn.Dropout(0.3),
                                    nn.Linear(512,2))
        self.vgg =nn.Sequential(self.vgg1,self.vgg2,self.vgg3,self.vgg4,
                                  self.vggfc1,self.vggfc2,self.vggfc3)

    def generate_src_mask(self, src):
        # 只需要源序列掩码（屏蔽填充符0），不需要tgt_mask
        # 形状：(batch_size, 1, 1, seq_length) → 适配MultiHeadAttention的输入s
        return (src != 0).unsqueeze(1).unsqueeze(2)
    def forward(self, src):
        # 1. 生成掩码
        src_mask = self.generate_src_mask(src)
       
        # 2. Encoder前向传播（提取序列特征）
        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))  # (batch, seq_len, d_model)
        enc_output = src_embedded
        for enc_layer in self.encoder_layers:
            enc_output = enc_layer(enc_output, src_mask)  # (batch, seq_len, d_model)
        enc_output = enc_output.unsqueeze(1)
        # 方案：均值池化（简单有效，适合序列分类），也可以用max_pool1d或取第一个token
        output = self.vgg(enc_output)
        return output


if __name__ == '__main__':
    writer = SummaryWriter('transformervgg_model_visualize')
    model = TransformerClassifier()
    writer.add_graph(model,(torch.randint(1,20,size=(64,128))))
    writer.close()
