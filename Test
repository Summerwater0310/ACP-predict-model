from model import *
from Bio import SeqIO
import numpy as np 
import torch
from torch import nn,optim
from torch.utils.data import Dataset,DataLoader
from tensorboardX import SummaryWriter
import os

def data_processing(data_dir):
    data_sequences = []
    data_labels = []
    for record in SeqIO.parse(data_dir, "fasta"):
        header = record.description
        sequence = str(record.seq)
        label = int(header.split("|")[1])
        data_sequences.append(sequence)
        data_labels.append(label)
    return data_sequences,data_labels

def encode_sequence(sequence, amino_acid_map):
    # 处理可能的未知氨基酸（不在map中的字符，默认映射为X=0）
    return [amino_acid_map.get(aa, 0) for aa in sequence]

class ProteinDataset(Dataset):
    def __init__(self, features, labels, max_seq_length=128):
        self.features = features  # 编码后的氨基酸序列（list of list）
        self.labels = labels      # 二分类标签（0/1）
        self.max_seq_length = max_seq_length
        self.pad_token = 0  # 填充符用X的编码（0）

    def __getitem__(self, index):
        seq = self.features[index]  # 单个变长序列（如[1,3,5,...,20]）
        label = self.labels[index]
    
        # 序列处理：截断过长，填充过短
        if len(seq) > self.max_seq_length:
            seq = seq[:self.max_seq_length]  # 截断到max_seq_length
        else:
            # 填充到max_seq_length（用pad_token=0）
            seq += [self.pad_token] * (self.max_seq_length - len(seq))
    
        # 转为Tensor（模型需要Tensor输入）
        seq_tensor = torch.tensor(seq, dtype=torch.long)
        label_tensor = torch.tensor(label, dtype=torch.long)  # 分类标签用long类型
    
        return seq_tensor, label_tensor

    def __len__(self):
        return len(self.features)

if __name__ == '__main__':
    amino_acid_map = {
        'X': 0, 'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,
        'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20
    }

    train_sequences,train_labels = data_processing('/home/zysun/transformer_project/Train_datasets_final.fasta')
    test_sequences,test_labels = data_processing('/home/zysun/transformer_project/Independent test dataset.fasta')

    encoded_train_sequences = [encode_sequence(seq, amino_acid_map) for seq in train_sequences]
    encoded_test_sequences = [encode_sequence(seq, amino_acid_map) for seq in test_sequences]

   

    train_datasets = ProteinDataset(
            features=encoded_train_sequences,
            labels=train_labels
        )
    train_dataloader = DataLoader(
            train_datasets,
            batch_size=64,
            shuffle=True,
            drop_last=False
            )
    test_datasets = ProteinDataset(
            features=encoded_test_sequences,
            labels=test_labels
        )
    test_dataloader = DataLoader(
            test_datasets,
            batch_size=64,  
            shuffle=False,
            drop_last=False
            )
    log_dir = './logs_transformervgg_train'
    os.system('rm -rf ./logs_transformervgg_train')
    writer = SummaryWriter(log_dir)

    total_train_size = len(train_datasets)
    total_test_size = len(test_datasets)


    device = torch.device('cuda:0')
    learning_rate = 1e-5
    epoch = 1000

    model = TransformerClassifier()
    model.to(device)
    loss_f = nn.CrossEntropyLoss() 
    loss_f = loss_f.to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        # 早停相关初始化
    best_test_acc = 0.0
    early_stop_patience = 10
    patience_counter = 0

    for i in range(epoch):
        print(f'----------epoch:{i+1},start!-------------------',flush=True)
        # 训练集累积变量初始化
        model.train()
        total_train_step = 0
        total_train_correct = 0
        total_train_samples = 0
        epoch_total_loss = 0

        for data in train_dataloader:
            optimizer.zero_grad()
            sequences, labels = data
            sequences = sequences.to(device)
            labels = labels.to(device)
            outputs = model(sequences)
            loss = loss_f(outputs, labels)
            loss.backward()
            optimizer.step()

            # 累积损失和样本数
            batch_size = labels.size(0)
            epoch_total_loss += loss.item() * batch_size
            total_train_samples += batch_size

            batch_correct = (outputs.argmax(1) == labels).sum()
            total_train_correct += batch_correct

            # 批次指标日志
            batch_accuracy = batch_correct / batch_size
            writer.add_scalar('Training Step Loss', loss, total_train_step)
            writer.add_scalar('Training Step Accuracy', batch_accuracy, total_train_step)

            total_train_step += 1
            if total_train_step % 100 == 0:
                print(f"Total step: {total_train_step}, Batch Loss: {loss}, Batch Accuracy: {batch_accuracy}",flush=True)

        # Epoch级训练指标
        epoch_avg_loss = epoch_total_loss / total_train_samples
        epoch_train_accuracy = total_train_correct / total_train_samples
        writer.add_scalar('Training Epoch Avg Loss', epoch_avg_loss, i+1)
        writer.add_scalar('Training Epoch Accuracy', epoch_train_accuracy, i+1)
        print(f'epoch{i+1}  avg_loss: {epoch_avg_loss:.4f}, epoch_accuracy: {epoch_train_accuracy:.4f}')

        # 测试集评估
        model.eval()
        total_test_loss = 0
        total_test_accuracy = 0
        total_test_samples = 0
        with torch.no_grad():
            for data in test_dataloader:
                sequences, labels = data
                sequences = sequences.to(device)
                labels = labels.to(device)
                outputs = model(sequences)
                loss = loss_f(outputs, labels)
                batch_size = labels.size(0)
                total_test_loss += loss.item() * batch_size
                total_test_samples += batch_size
                total_test_accuracy += (outputs.argmax(1) == labels).sum()

        # 测试集指标
        test_avg_loss = total_test_loss / total_test_samples
        test_acc = total_test_accuracy / total_test_samples
        writer.add_scalar('Test Avg Loss', test_avg_loss, i+1)
        writer.add_scalar('Test Accuracy', test_acc, i+1)
        print(f'(Testset) avg loss: {test_avg_loss}, accuracy: {test_acc}',flush=True)


    writer.close()
